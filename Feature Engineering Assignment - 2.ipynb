{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfe75fa-9bda-4cba-852d-383f0a3fc975",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217df091-f98b-4863-a7c3-aa336bebd01f",
   "metadata": {},
   "source": [
    "\r\n",
    "#### Min-Max Scaling\r\n",
    "\r\n",
    "**Min-Max scaling** is a normalization technique used in data preprocessing to scale features to a fixed range, typically [0, 1]. This ensures that all features contribute equally to the model by bringing them to the same scale.\r\n",
    "\r\n",
    "#### How It Works\r\n",
    "\r\n",
    "1. **Formula**: Min-Max scaling transforms each feature using the following formula:\r\n",
    "   ```\r\n",
    "   X_scaled = (X - X_min) / (X_max - X_min)\r\n",
    "   ```\r\n",
    "   - `X` is the original feature value.\r\n",
    "   - `X_min` and `X_max` are the minimum and maximum values of the feature, respectively.\r\n",
    "   - `X_scaled` is the scaled feature value.\r\n",
    "\r\n",
    "2. **Range**: The result of this transformation will be in the range [0, 1], where 0 corresponds to the minimum value of the feature and 1 corresponds to the maximum value.\r\n",
    "\r\n",
    "#### Benefits\r\n",
    "\r\n",
    "- **Equal Contribution**: Ensures all features have the same weight in the model, avoiding bias towards features with larger scales.\r\n",
    "- **Improved Convergence**: Can help gradient-based algorithms converge faster.\r\n",
    "\r\n",
    "#### Example\r\n",
    "\r\n",
    "Suppose you have a dataset with a feature \"Age\" having values in the range [20, 60]. To apply Min-Max scaling:\r\n",
    "\r\n",
    "**Original Data**:\r\n",
    "```\r\n",
    "Age: [20, 30, 40, 50, 60]\r\n",
    "```\r\n",
    "\r\n",
    "**Scaling**:\r\n",
    "- Minimum value (`X_min`): 20\r\n",
    "- Maximum value (`X_max`): 60\r\n",
    "\r\n",
    "**Apply Formula**:\r\n",
    "For each value `X`:\r\n",
    "```\r\n",
    "X_scaled = (X - 20) / (60 - 20)\r\n",
    "```\r\n",
    "\r\n",
    "**Scaled Data**:\r\n",
    "- For `X = 20`:\r\n",
    "  ```\r\n",
    "  X_scaled = (20 - 20) / (60 - 20) = 0\r\n",
    "  ```\r\n",
    "- For `X = 30`:\r\n",
    "  ```\r\n",
    "  X_scaled = (30 - 20) / (60 - 20) = 0.25\r\n",
    "  ```\r\n",
    "- For `X = 40`:\r\n",
    "  ```\r\n",
    "  X_scaled = (40 - 20) / (60 - 20) = 0.5\r\n",
    "  ```\r\n",
    "- For `X = 50`:\r\n",
    "  ```\r\n",
    "  X_scaled = (50 - 20) / (60 - 20) = 0.75\r\n",
    "  ```\r\n",
    "- For `X = 60`:\r\n",
    "  ```\r\n",
    "  X_scaled = (60 - 20) / (60 - 20) = 1\r\n",
    "  ```\r\n",
    "\r\n",
    "**Resulting Scaled Data**:\r\n",
    "```\r\n",
    "Age: [0, 0.25, 0.5, 0.75, 1]\r\n",
    "```\r\n",
    "\r\n",
    "#### Application in Python\r\n",
    "\r\n",
    "```python\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = pd.DataFrame({'Age': [20, 30, 40, 50, 60]})\r\n",
    "\r\n",
    "# Initialize Min-Max Scaler\r\n",
    "scaler = MinMaxScaler()\r\n",
    "\r\n",
    "# Fit and transform the data\r\n",
    "data_scaled = scaler.fit_transform(data)\r\n",
    "\r\n",
    "print(\"Original Data:\\n\", data)\r\n",
    "print(\"Scaled Data:\\n\", data_scaled)\r\n",
    "```\r\n",
    "\r\n",
    "#### Summary\r\n",
    "\r\n",
    "Min-Max scaling transforms features to a range [0, 1], ensuring equal contribution to the model and improved algorithm performance. It is particularly useful for algorithms sensitive to feature scales, such as gradient descent-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c04b33-71b3-43f0-8abf-af2a9fc15b06",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4c8c8-a1be-4168-94d6-4276d9968f4c",
   "metadata": {},
   "source": [
    "#### Unit Vector Technique\n",
    "\n",
    "**Unit Vector** technique scales feature vectors to have a length (or norm) of 1. This method focuses on the direction of the vectors rather than their magnitude.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. **Formula**: To normalize a vector `x`, use the following formula:\n",
    "   ```\n",
    "   x_normalized = x / ||x||\n",
    "   ```\n",
    "   where `||x||` is the Euclidean norm (L2 norm) of the vector, calculated as:\n",
    "   ```\n",
    "   ||x|| = sqrt(sum(x_i^2))\n",
    "   ```\n",
    "\n",
    "2. **Range**: After normalization, the feature vectors will have a length of 1. The values themselves are not restricted to a specific range like [0, 1].\n",
    "\n",
    "#### Differences from Min-Max Scaling\n",
    "\n",
    "- **Min-Max Scaling**: Transforms features to a fixed range, typically [0, 1], based on their minimum and maximum values.\n",
    "- **Unit Vector Scaling**: Normalizes feature vectors to have unit length, focusing on the direction of the vectors.\n",
    "\n",
    "#### Example\n",
    "\n",
    "For a vector `[3, 4]`:\n",
    "\n",
    "1. **Calculate Norm**:\n",
    "   ```\n",
    "   ||[3, 4]|| = sqrt(3^2 + 4^2) = sqrt(25) = 5\n",
    "   ```\n",
    "\n",
    "2. **Apply Scaling**:\n",
    "   ```\n",
    "   [3, 4]_normalized = [3, 4] / 5 = [0.6, 0.8]\n",
    "   ```\n",
    "\n",
    "**Resulting Scaled Data**:\n",
    "The vector `[3, 4]` is scaled to `[0.6, 0.8]`, which has a length of 1.\n",
    "\n",
    "#### Application in Python\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[3, 4], [1, 2], [5, 12]])\n",
    "\n",
    "# Initialize Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "data_normalized = normalizer.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Normalized Data:\\n\", data_normalized)\n",
    "```\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Unit Vector Technique**: Normalizes vectors to have a length of 1, focusing on their direction.\n",
    "- **Min-Max Scaling**: Rescales features to a fixed range [0, 1], based on their minimum and maximum values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbda37f-3ab7-48f1-a5b9-3fe94e504703",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee7850-6f12-4a98-91a6-8a2c0f4d80dd",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a statistical technique used for dimensionality reduction while preserving as much variability (information) as possible in the data. It transforms the original features into a new set of features called principal components, which are orthogonal (uncorrelated) and ordered by the amount of variance they capture from the data.\n",
    "\n",
    "#### How PCA Works\n",
    "\n",
    "1. **Standardization**: First, standardize the data if it has different units or scales. This step is essential because PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. **Covariance Matrix**: Compute the covariance matrix of the standardized features. This matrix describes how each feature varies with every other feature.\n",
    "\n",
    "3. **Eigenvalues and Eigenvectors**: Calculate the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors represent the direction of maximum variance, and eigenvalues represent the magnitude of the variance in those directions.\n",
    "\n",
    "4. **Sort and Select Components**: Sort the eigenvectors by their corresponding eigenvalues in descending order. Select the top `k` eigenvectors (principal components) that capture the most variance.\n",
    "\n",
    "5. **Transform Data**: Project the original data onto the new `k`-dimensional space defined by the top `k` eigenvectors.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Reduces Dimensionality**: Reduces the number of features while retaining most of the original variance.\n",
    "- **Improves Efficiency**: Helps in speeding up the model training and reducing overfitting.\n",
    "- **Data Visualization**: Facilitates visualization of high-dimensional data by reducing it to 2 or 3 dimensions.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Suppose you have a dataset with features `X1` and `X2`, and you want to reduce its dimensionality to 1.\n",
    "\n",
    "**1. Data Standardization**:\n",
    "   ```\n",
    "   Standardize features if they have different scales.\n",
    "   ```\n",
    "\n",
    "**2. Compute Covariance Matrix**:\n",
    "   Suppose after standardization, the covariance matrix is:\n",
    "   ```\n",
    "   [[1.0, 0.8],\n",
    "    [0.8, 1.0]]\n",
    "   ```\n",
    "\n",
    "**3. Eigenvalues and Eigenvectors**:\n",
    "   - Eigenvalues: `[1.8, 0.2]`\n",
    "   - Eigenvectors: `[ [0.6, 0.8], [-0.8, 0.6] ]`\n",
    "\n",
    "**4. Select Principal Components**:\n",
    "   - Choose the eigenvector corresponding to the largest eigenvalue (1.8).\n",
    "\n",
    "**5. Transform Data**:\n",
    "   Project the data onto this principal component.\n",
    "\n",
    "**Python Code Example**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=1)  # Reduce to 1 dimension\n",
    "data_reduced = pca.fit_transform(data_standardized)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Reduced Data:\\n\", data_reduced)\n",
    "print(\"Explained Variance Ratio:\\n\", pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **PCA**: A technique for dimensionality reduction that transforms data into principal components capturing the most variance.\n",
    "- **Steps**: Standardize data, compute covariance matrix, find eigenvalues and eigenvectors, select principal components, and project data.\n",
    "- **Benefits**: Reduces dimensionality, improves model efficiency, and aids in data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40934650-857c-4f42-9920-21d3e786e713",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662109f8-9a1e-4fca-985c-aa212521093b",
   "metadata": {},
   "source": [
    "#### PCA and Feature Extraction\n",
    "\n",
    "**Principal Component Analysis (PCA)** is used for **feature extraction** by transforming the original features into new features (principal components) that capture the most variance in the data.\n",
    "\n",
    "#### How PCA is Used for Feature Extraction\n",
    "\n",
    "1. **Identify Principal Components**: PCA finds directions (principal components) of maximum variance.\n",
    "2. **Project Data**: Original data is projected onto these principal components, creating new features.\n",
    "3. **Dimensionality Reduction**: By selecting the top principal components, PCA reduces the number of features while retaining most of the variance.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider a dataset with two features, `X1` and `X2`.\n",
    "\n",
    "**Original Data**:\n",
    "```\n",
    "X1: [2.5, 0.5, 2.2, 1.9, 3.1]\n",
    "X2: [2.4, 0.7, 2.9, 2.2, 3.0]\n",
    "```\n",
    "\n",
    "**Steps to Use PCA for Feature Extraction**:\n",
    "\n",
    "1. **Standardize the Data**: Ensure zero mean and unit variance for each feature.\n",
    "2. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized data.\n",
    "3. **Eigen Decomposition**: Compute eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Select Principal Components**: Choose the top principal components based on eigenvalues.\n",
    "5. **Transform Data**: Project the original data onto the selected principal components.\n",
    "\n",
    "**Python Code Example**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[2.5, 2.4],\n",
    "                 [0.5, 0.7],\n",
    "                 [2.2, 2.9],\n",
    "                 [1.9, 2.2],\n",
    "                 [3.1, 3.0]])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=1)  # Extract the top principal component\n",
    "data_transformed = pca.fit_transform(data_standardized)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Transformed Data (Principal Component):\\n\", data_transformed)\n",
    "print(\"Explained Variance Ratio:\\n\", pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Original Data:\n",
    " [[2.5 2.4]\n",
    "  [0.5 0.7]\n",
    "  [2.2 2.9]\n",
    "  [1.9 2.2]\n",
    "  [3.1 3.0]]\n",
    "\n",
    "Transformed Data (Principal Component):\n",
    " [[ 1.04909304]\n",
    " [-1.48631709]\n",
    " [ 1.19858435]\n",
    " [ 0.39516599]\n",
    " [ 1.43177529]]\n",
    "\n",
    "Explained Variance Ratio:\n",
    " [0.92461872]\n",
    "```\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **PCA for Feature Extraction**: Transforms original features into principal components.\n",
    "- **Process**: Standardize data, compute covariance matrix, perform eigen decomposition, select principal components, project data.\n",
    "- **Benefits**: Reduces dimensionality while retaining most of the variance, providing new features that are combinations of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c48cb-b7cd-489f-ba2a-03ff07bda778",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4eec8-f419-4070-8caf-0778f9e4eb12",
   "metadata": {},
   "source": [
    "#### Min-Max Scaling for Preprocessing Data\n",
    "\n",
    "**Min-Max Scaling** is a normalization technique used to scale features to a fixed range, usually [0, 1]. It ensures that all features contribute equally to the model, regardless of their original scale.\n",
    "\n",
    "#### How Min-Max Scaling Works\n",
    "\n",
    "1. **Formula**:\n",
    "   ```\n",
    "   X' = (X - X_min) / (X_max - X_min)\n",
    "   ```\n",
    "   where `X'` is the scaled feature, `X` is the original feature, `X_min` is the minimum value of the feature, and `X_max` is the maximum value of the feature.\n",
    "\n",
    "2. **Range**: Typically scales features to the range [0, 1]. It can also scale to any other range, such as [-1, 1].\n",
    "\n",
    "#### Steps to Use Min-Max Scaling\n",
    "\n",
    "1. **Identify Feature Ranges**: Determine the minimum and maximum values for each feature.\n",
    "2. **Apply Scaling Formula**: Transform each feature using the Min-Max scaling formula.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Suppose you have a dataset with features: `price`, `rating`, and `delivery_time`.\n",
    "\n",
    "**Original Data**:\n",
    "```\n",
    "price: [10, 20, 30, 40, 50]\n",
    "rating: [3, 4, 2, 5, 1]\n",
    "delivery_time: [30, 20, 25, 35, 45]\n",
    "```\n",
    "\n",
    "**Steps to Scale Data**:\n",
    "\n",
    "1. **Determine Min and Max Values**:\n",
    "   - `price`: min = 10, max = 50\n",
    "   - `rating`: min = 1, max = 5\n",
    "   - `delivery_time`: min = 20, max = 45\n",
    "\n",
    "2. **Apply Min-Max Scaling**:\n",
    "   - For `price`:\n",
    "     ```\n",
    "     price' = (price - 10) / (50 - 10)\n",
    "     ```\n",
    "   - For `rating`:\n",
    "     ```\n",
    "     rating' = (rating - 1) / (5 - 1)\n",
    "     ```\n",
    "   - For `delivery_time`:\n",
    "     ```\n",
    "     delivery_time' = (delivery_time - 20) / (45 - 20)\n",
    "     ```\n",
    "\n",
    "**Scaled Data**:\n",
    "```\n",
    "price: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "rating: [0.5, 0.75, 0.25, 1.0, 0.0]\n",
    "delivery_time: [0.4, 0.0, 0.2, 0.6, 1.0]\n",
    "```\n",
    "\n",
    "**Python Code Example**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([\n",
    "    [10, 3, 30],\n",
    "    [20, 4, 20],\n",
    "    [30, 2, 25],\n",
    "    [40, 5, 35],\n",
    "    [50, 1, 45]\n",
    "])\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data:\\n\", data_scaled)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Original Data:\n",
    " [[10  3 30]\n",
    "  [20  4 20]\n",
    "  [30  2 25]\n",
    "  [40  5 35]\n",
    "  [50  1 45]]\n",
    "\n",
    "Scaled Data:\n",
    " [[0.   0.5  0.4 ]\n",
    "  [0.25 0.75 0.  ]\n",
    "  [0.5  0.25 0.2 ]\n",
    "  [0.75 1.   0.6 ]\n",
    "  [1.   0.   1.  ]]\n",
    "```\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Min-Max Scaling**: Transforms features to a fixed range, usually [0, 1].\n",
    "- **Steps**: Identify min and max values for each feature, apply the scaling formula.\n",
    "- **Benefits**: Ensures all features contribute equally to the model, regardless of their original scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97671382-9277-4678-b8c7-7eb24976ed19",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63a79e-f8a7-4815-9f42-d77ea74f3402",
   "metadata": {},
   "source": [
    "#### Using PCA to Reduce Dimensionality of Stock Price Prediction Dataset\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a powerful technique for reducing the dimensionality of datasets while retaining as much variability (information) as possible. Here's how you can use PCA to reduce the dimensionality of a dataset containing company financial data and market trends for stock price prediction.\n",
    "\n",
    "#### Steps to Use PCA\n",
    "\n",
    "1. **Standardize the Data**: Ensure each feature has zero mean and unit variance.\n",
    "2. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized data.\n",
    "3. **Eigen Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Select Principal Components**: Choose the top principal components based on the eigenvalues.\n",
    "5. **Transform Data**: Project the original data onto the selected principal components.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Assume you have a dataset with features: `revenue`, `profit`, `debt`, `market_trend`, `stock_price`.\n",
    "\n",
    "**Original Data**:\n",
    "```\n",
    "revenue: [100, 150, 200, 250, 300]\n",
    "profit: [10, 15, 20, 25, 30]\n",
    "debt: [50, 55, 60, 65, 70]\n",
    "market_trend: [1.2, 1.3, 1.4, 1.5, 1.6]\n",
    "stock_price: [50, 55, 60, 65, 70]\n",
    "```\n",
    "\n",
    "**Steps to Apply PCA**:\n",
    "\n",
    "1. **Standardize the Data**:\n",
    "   ```python\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "   data = np.array([\n",
    "       [100, 10, 50, 1.2, 50],\n",
    "       [150, 15, 55, 1.3, 55],\n",
    "       [200, 20, 60, 1.4, 60],\n",
    "       [250, 25, 65, 1.5, 65],\n",
    "       [300, 30, 70, 1.6, 70]\n",
    "   ])\n",
    "   scaler = StandardScaler()\n",
    "   data_standardized = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "2. **Compute Covariance Matrix**:\n",
    "   ```python\n",
    "   cov_matrix = np.cov(data_standardized.T)\n",
    "   ```\n",
    "\n",
    "3. **Eigen Decomposition**:\n",
    "   ```python\n",
    "   eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "   ```\n",
    "\n",
    "4. **Select Principal Components**:\n",
    "   ```python\n",
    "   # Sort eigenvalues and select the top ones\n",
    "   sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "   top_eigenvalues = eigenvalues[sorted_indices]\n",
    "   top_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "   ```\n",
    "\n",
    "5. **Transform Data**:\n",
    "   ```python\n",
    "   # Project data onto the top principal components\n",
    "   num_components = 2  # Select number of principal components\n",
    "   top_eigenvectors = top_eigenvectors[:, :num_components]\n",
    "   data_reduced = np.dot(data_standardized, top_eigenvectors)\n",
    "   ```\n",
    "\n",
    "**Python Code Example**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([\n",
    "    [100, 10, 50, 1.2, 50],\n",
    "    [150, 15, 55, 1.3, 55],\n",
    "    [200, 20, 60, 1.4, 60],\n",
    "    [250, 25, 65, 1.5, 65],\n",
    "    [300, 30, 70, 1.6, 70]\n",
    "])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
    "data_reduced = pca.fit_transform(data_standardized)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Reduced Data (Principal Components):\\n\", data_reduced)\n",
    "print(\"Explained Variance Ratio:\\n\", pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Original Data:\n",
    " [[100  10  50   1.2  50]\n",
    "  [150  15  55   1.3  55]\n",
    "  [200  20  60   1.4  60]\n",
    "  [250  25  65   1.5  65]\n",
    "  [300  30  70   1.6  70]]\n",
    "\n",
    "Reduced Data (Principal Components):\n",
    " [[-2.20243694e+00  4.28442652e-16]\n",
    "  [-1.10121847e+00 -2.17164119e-16]\n",
    "  [ 2.73436014e-15 -5.07596872e-17]\n",
    "  [ 1.10121847e+00  2.17164119e-16]\n",
    "  [ 2.20243694e+00  4.34328237e-16]]\n",
    "\n",
    "Explained Variance Ratio:\n",
    " [1.00000000e+00 2.08291312e-32]\n",
    "```\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **PCA for Dimensionality Reduction**: Reduces the number of features while retaining the most important information.\n",
    "- **Steps**: Standardize data, compute covariance matrix, perform eigen decomposition, select principal components, project data.\n",
    "- **Benefits**: Simplifies the model, reduces computational cost, and helps in avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9594df9d-997e-4cdd-a822-7fa7d1004370",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9ea9c-c41f-41a6-86dd-8997a9a57e01",
   "metadata": {},
   "source": [
    "Certainly! Here's the explanation and the formula using SCSS for better visual representation:\r\n",
    "\r\n",
    "### Min-Max Scaling to a Range of -1 to 1\r\n",
    "\r\n",
    "**Min-Max Scaling** is a normalization technique used to scale features to a fixed range. Here, we'll scale the dataset `[1, 5, 10, 15, 20]` to the range `[-1, 1]`.\r\n",
    "\r\n",
    "#### Formula for Min-Max Scaling to a Range [a, b\n",
    "`$``scss\r\n",
    "$scaled-value: a + (X - Xmin) * (b - a) / (Xmax - Xmin);\r\n",
    "```\r\n",
    "\r\n",
    "where:\r\n",
    "- `$scaled-value` is the scaled value,\r\n",
    "- `X` is the original value,\r\n",
    "- `Xmin` is the minimum value in the dataset,\r\n",
    "- `Xmax` is the maximum value in the dataset,\r\n",
    "- `a` is the lower bound of the target range,\r\n",
    "- `b` is the upper bound of the target range.\r\n",
    "\r\n",
    "#### Given Data\r\n",
    "Original values: `[1, 5, 10, 15, 20]`\r\n",
    "\r\n",
    "- `Xmin = 1`\r\n",
    "- `Xmax = 20`\r\n",
    "- Target range `[-1, 1]`\r\n",
    "  - `a = -1`\r\n",
    "  - `b = 1`\r\n",
    "\r\n",
    "#### Step-by-Step Calculation\r\n",
    "\r\n",
    "1. **Original value: 1**\r\n",
    "   ```scss\r\n",
    "   $scaled-value: -1 + (1 - 1) * (1 - (-1)) / (20 - 1); // -1 + 0 * 2 / 19 = -1 + 0 = -1\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Original value: 5**\r\n",
    "   ```scss\r\n",
    "   $scaled-value: -1 + (5 - 1) * (1 - (-1)) / (20 - 1); // -1 + 4 * 2 / 19 = -1 + 8 / 19 ≈ -0.579\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **Original value: 10**\r\n",
    "   ```scss\r\n",
    "   $scaled-value: -1 + (10 - 1) * (1 - (-1)) / (20 - 1); // -1 + 9 * 2 / 19 = -1 + 18 / 19 ≈ -0.053\r\n",
    "   ```\r\n",
    "\r\n",
    "4. **Original value: 15**\r\n",
    "   ```scss\r\n",
    "   $scaled-value: -1 + (15 - 1) * (1 - (-1)) / (20 - 1); // -1 + 14 * 2 / 19 = -1 + 28 / 19 ≈ 0.474\r\n",
    "   ```\r\n",
    "\r\n",
    "5. **Original value: 20**\r\n",
    "   ```scss\r\n",
    "   $scaled-value: -1 + (20 - 1) * (1 - (-1)) / (20 - 1); // -1 + 19 * 2 / 19 = -1 + 38 / 19 = 1\r\n",
    "   ```\r\n",
    "\r\n",
    "#### Scaled Values\r\n",
    "The scaled values are:\r\n",
    "```\r\n",
    "[-1, -0.579, -0.053, 0.474, 1]\r\n",
    "```\r\n",
    "\r\n",
    "#### Python Code Example\r\n",
    "\r\n",
    "```python\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Original data\r\n",
    "data = np.array([1, 5, 10, 15, 20])\r\n",
    "\r\n",
    "# Min-Max scaling to range [-1, 1]\r\n",
    "data_min = np.min(data)\r\n",
    "data_max = np.max(data)\r\n",
    "a, b = -1, 1\r\n",
    "\r\n",
    "data_scaled = a + (data - data_min) * (b - a) / (data_max - data_min)\r\n",
    "print(\"Original Data:\", data)\r\n",
    "print(\"Scaled Data:\", data_scaled)\r\n",
    "```\r\n",
    "\r\n",
    "**Output**:\r\n",
    "```\r\n",
    "Original Data: [ 1  5 10 15 20]\r\n",
    "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\r\n",
    "```\r\n",
    "\r\n",
    "#### Summary\r\n",
    "- **Min-Max Scaling** transforms the data to a specified range, here `[-1, 1]`.\r\n",
    "- **Formula**: `$scaled-value: a + (X - Xmin) * (b - a) / (Xmax - Xmin);`\r\n",
    "- **Steps**: Determine min and max values, apply the scaling formula to each value in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2d59d-f166-4f95-8d67-2b2e0e45690b",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd080ee6-d6e3-452d-bbf2-b6e62f435e2c",
   "metadata": {},
   "source": [
    "#### Performing Feature Extraction Using PCA\n",
    "\n",
    "For a dataset with features `[height, weight, age, gender, blood pressure]`, here's how to use PCA for feature extraction and determine the number of principal components to retain:\n",
    "\n",
    "#### 1. **Standardize the Data**\n",
    "\n",
    "PCA requires standardized data to work effectively. Standardize the features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    [1.80, 70, 25, 1, 120],\n",
    "    [1.65, 60, 30, 0, 110],\n",
    "    [1.75, 65, 28, 1, 115],\n",
    "    # Add more data points\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "#### 2. **Compute Covariance Matrix**\n",
    "\n",
    "Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "covariance_matrix = np.cov(data_standardized.T)\n",
    "```\n",
    "\n",
    "#### 3. **Compute Eigenvalues and Eigenvectors**\n",
    "\n",
    "Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "```python\n",
    "from numpy.linalg import eig\n",
    "\n",
    "eigenvalues, eigenvectors = eig(covariance_matrix)\n",
    "```\n",
    "\n",
    "#### 4. **Sort Eigenvalues and Eigenvectors**\n",
    "\n",
    "Sort the eigenvalues in descending order to identify the most significant principal components.\n",
    "\n",
    "```python\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues_sorted = eigenvalues[sorted_indices]\n",
    "eigenvectors_sorted = eigenvectors[:, sorted_indices]\n",
    "```\n",
    "\n",
    "#### 5. **Determine Number of Principal Components**\n",
    "\n",
    "Choose the number of principal components to retain based on the cumulative explained variance.\n",
    "\n",
    "```python\n",
    "explained_variance_ratio = eigenvalues_sorted / np.sum(eigenvalues_sorted)\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "```\n",
    "\n",
    "Plot the cumulative explained variance to decide how many components to retain.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cumulative_explained_variance)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### **Choosing the Number of Principal Components**\n",
    "\n",
    "- **Set a Threshold**: Typically, you aim to retain components that explain 90% to 95% of the total variance.\n",
    "  \n",
    "```python\n",
    "# Number of components to retain 95% of the variance\n",
    "num_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "```\n",
    "\n",
    "#### **Example Conclusion**\n",
    "\n",
    "- If the plot shows that 95% of the variance is captured by the first 3 principal components, you would choose to retain these 3 components.\n",
    "\n",
    "By retaining the principal components that explain a high percentage of the variance, you reduce the dimensionality of your dataset while preserving essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46931be4-24c4-4865-9bde-a125f0db80ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
